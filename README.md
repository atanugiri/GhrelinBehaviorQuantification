# GhrelinBehaviorQuantification

Short README for the `GhrelinBehaviorQuantification` analysis project.

## Project overview

- Purpose: analysis pipeline for DeepLabCut-derived behavioral tracking data related to ghrelin experiments. Contains Jupyter notebooks for exploratory data analysis and Python scripts to extract/compute features and insert them into a database.

## Repository layout (important items)

- `environment.yml` : conda environment used for reproducible environments.
- `DLC-Jupyter-Notebooks/` : analysis notebooks (e.g., `40_data_analysis_angle_features.ipynb`).
- `Python_scripts/` : reusable scripts and modules including:
  - `config.py` : provides `get_conn()` and `get_data_dir()` used to connect to Postgres or fallback to CSV.
  - `Data_analysis/`, `Feature_functions/`, `Extract_db_columns/`, `Insert_to_featuretable/`, `Utility_functions/` : core analysis utilities and helpers.
- `DATA_DIR` (not a repo file): data directory referenced by `config.get_data_dir()` (used as CSV fallback).

## Quickstart (local)

1. Create and activate conda environment:

```bash
conda env create -f environment.yml
conda activate <env-name-from-yml>
```

2. Start Jupyter (lab or notebook) from repo root and open notebooks in `DLC-Jupyter-Notebooks/`:

```bash
jupyter lab
# or
jupyter notebook
```

3. Notebook notes:
- Notebooks add the project root to `sys.path` (see the top cell of notebooks like `40_data_analysis_angle_features.ipynb`) so scripts in `Python_scripts/` import cleanly.
- Notebooks attempt to connect to a Postgres DB via `Python_scripts.config.get_conn()` and on failure fall back to CSV files found under `DATA_DIR` (configured in `config.py`).

## Configuration and data

- Check `Python_scripts/config.py` for database credentials and data directory resolution. The notebooks handle a fallback to CSVs named like `dlc_table_*.csv` located under `DATA_DIR`.
- If you don't have DB access, place the CSV exports under the path returned by `config.get_data_dir()` or update `config.py` to point to a local folder with the expected CSVs.

### Data location (recommended)

- Repository-local placeholder: add project data under the repo `data/` directory (this repository contains a placeholder `data/` folder). The repository expects CSV exports of the main metadata table named like `dlc_table_*.csv` (for example `dlc_table_saline.csv`, `dlc_table_ghrelin.csv`) to be placed in `data/` or under the path returned by `Python_scripts.config.get_data_dir()`.
- PostgreSQL table: for long-term archival we recommend exporting the `dlc_table` table from PostgreSQL into `data/dlc_table_YYYYMMDD.csv` and committing metadata-only CSVs (not raw videos) or storing raw video files outside the repository (see below).
- Large data (videos): raw video files should NOT be checked into the git repository. Store large video files on a shared drive, object storage (S3), or lab server and put a small metadata CSV in `data/` that references those locations. Use `Python_scripts.resolve_video_path()` (notebooks) to map stored video paths to local copies.

## Important notebooks

- `DLC-Jupyter-Notebooks/40_data_analysis_angle_features.ipynb` — calculates and plots angle features and exports summary Excel files.
- `DLC-Jupyter-Notebooks/30_data_analysis.ipynb` and related notebooks — other analyses and visualizations.

### Code location

- All code is under `Python_scripts/`. Key modules used by the analyses include:
  - `Python_scripts/Feature_functions/trajectory_curvature.py` — curvature feature implementation.
  - `Python_scripts/Data_analysis/compute_binned_curvature_stats.py` — curvature analysis helpers.
  - `Python_scripts/Insert_to_featuretable/insert_trajectory_curvature.py` — utilities to insert curvature features into a database.
  - Notebooks import these modules by adding the repository root to `sys.path` (see the top cell in notebooks such as `DLC-Jupyter-Notebooks/37_data_analysis_curvature.ipynb`).

### Statistical outputs

- Statistical outputs (figures, summary tables, Excel exports) are generated by the notebooks and saved by each notebook in the current working directory when executed from the project root. Typical outputs include PDF figures (e.g., `White_Modulation_2X_{task_name}_ang_likelihood_sweep.pdf`) and Excel files (e.g., `10X_White_Angle.xlsx`). When running notebooks non-interactively, the outputs will appear in the notebook's working folder (run from repository root to preserve expected paths).

### Reproducibility notes (data/code/outputs)

- To reproduce: clone the repository, change to the project root, create the conda environment from `environment.yml`, and run the notebooks from the project root so imports from `Python_scripts/` resolve.
- Do not publish absolute local paths (for example `/Users/atanugiri/...`) in the manuscript; use repository-relative paths such as `DLC-Jupyter-Notebooks/37_data_analysis_curvature.ipynb` and `Python_scripts/Feature_functions/trajectory_curvature.py` instead.

## Development notes

- To run individual scripts, use `python` from the repository root so imports from `Python_scripts` resolve correctly.
- Consider adding a `requirements.txt` or a pinned `environment.yml` environment name for easier activation.

## Recommended next steps

- Add `CONTRIBUTING.md` with development guidelines (how to run notebooks, format code, run tests).
- Add `DB_SETUP.md` with step-by-step database credentials, connection, and how to generate CSV fallbacks.
- Optionally add a small `README-notebooks.md` describing each notebook (purpose and expected inputs/outputs).

## Contact

- Repository owner / maintainer: `atanugiri` (local workspace)

---

(Generated automatically — tell me if you want more detail in any section or want me to also create `CONTRIBUTING.md` and `DB_SETUP.md`.)